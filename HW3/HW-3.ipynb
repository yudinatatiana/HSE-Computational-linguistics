{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. \n",
    "\n",
    "Реализуйте алгоритм Symspell - https://medium.com/@wolfgarbe/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам в словаре применяется только одна операция - удаление символа. Чтобы найти исправление из слова тоже удаляются символы и сравниваются с теми, что хранятся в словаре. Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from pprint import pprint\n",
    "from nltk import sent_tokenize\n",
    "punctuation += \"«»—…“”\"\n",
    "punct = set(punctuation)\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    sents = sentenize(text)\n",
    "    return [normalize(sent.text) for sent in sents]\n",
    "\n",
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    tokens = [token for token in tokens]\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(tuple(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad = open('data/sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('data/correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию, которая будет сопоставлять слова в правильном и ошибочном варианте\n",
    "# разобьем предложение по пробелам и удалим пунктуация на границах слов\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
    "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('апофеозом', 'опофеозом'),\n",
      " ('дня', 'дня'),\n",
      " ('для', 'для'),\n",
      " ('меня', 'меня'),\n",
      " ('сегодня', 'сегодня'),\n",
      " ('стала', 'стала'),\n",
      " ('фраза', 'фраза'),\n",
      " ('услышанная', 'услышанная'),\n",
      " ('в', 'в'),\n",
      " ('новостях', 'новостях')]\n"
     ]
    }
   ],
   "source": [
    "pprint(align_words(true[1], bad[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = open('data/wiki_data.txt', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем словарь\n",
    "vocab = set(re.findall('\\w+', corpus.lower()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mistaken(word, vocab):\n",
    "\n",
    "    if word in vocab:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = Counter(re.findall('\\w+', corpus.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# фунцкия расчета вероятности слова\n",
    "N = sum(WORDS.values())\n",
    "def P(word, N=N): \n",
    "    \"Вычисляем вероятность слова\"\n",
    "    return WORDS[word] / N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word, precalculation_dict): \n",
    "    \"Находим наиболее вероятное похожее слово\"\n",
    "    return max(candidates(word, precalculation_dict), key=P)\n",
    "\n",
    "def candidates(word, precalculation_dict): \n",
    "    \"Генерируем кандидатов на исправление\"\n",
    "    '''Generate terms with an edit distance (deletes only) from the input term and search them in the dictionary. \n",
    "    For a word of length n, an alphabet size of a, an edit distance of 1, there will be just n deletions, \n",
    "    for a total of n terms at search time.'''\n",
    "    candidates = list()\n",
    "    candidates.append(word)\n",
    "    candidates.extend(precalculation_dict[word])\n",
    "    deletes = edits1(word)\n",
    "    for one_delete in deletes:\n",
    "        candidates.extend(precalculation_dict[one_delete])\n",
    "    return candidates\n",
    "\n",
    "def edits1(word):\n",
    "    \"Создаем кандидатов, которые отличаются на одну букву\"\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    return set(deletes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Generate terms with an edit distance (deletes only) from each dictionary term and add them together with the original term to the dictionary. This has to be done only once during a pre-calculation step.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "precalculation_dict = defaultdict(list)\n",
    "\n",
    "for word in vocab:\n",
    "    deletes = edits1(word)\n",
    "    for one_delete in deletes:\n",
    "        precalculation_dict[one_delete].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['солнца', 'солнце', 'солнцу']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precalculation_dict['солнц']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 87.7 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'конце'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "correction('сонце', precalculation_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 52.9 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'апофеоз'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "correction('опофеоз', precalculation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки используем будем использовать три метрики:  \n",
    "1) процент правильных слов;  \n",
    "2) процент исправленных ошибок  \n",
    "3) процент ошибочно исправленных правильных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        # чтобы два раза не исправлять одно и тоже слово - закешируем его\n",
    "        # перед тем как считать исправление проверим нет ли его в кеше\n",
    "        predicted = cashed.get(pair[1], correction(pair[1], precalculation_dict))\n",
    "        cashed[pair[1]] = predicted\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "        \n",
    "    if not i % 100:\n",
    "        print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4312687312687313\n",
      "0.3200306983883346\n",
      "0.5520845296887562\n"
     ]
    }
   ],
   "source": [
    "print(correct/total) \n",
    "print(mistaken_fixed/total_mistaken) \n",
    "print(correct_broken/total_correct) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На семинаре: (пересчитала с исправленной строчкой cashed[pair[1]] = predicted)\n",
    "\n",
    "0.8693306693306694\n",
    "\n",
    "0.5042210283960092\n",
    "\n",
    "0.07603077983231882\n",
    "\n",
    "Таким образом, удалось исправить меньше ошибок (32 < 50), меньше слов было подобрано правильно (43 < 87) и было сделано больше ошибочных исправлений (55 > 8). Но текущий алгоритм работает значительно быстрее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. \n",
    "Добавьте к полученному алгоритму исправления (Symspell) триграммную модель и проверьте, улучшает ли она качество. Триграммную модель нужно вставить туда, где у вас выбирается один из нескольких кандидатов на исправление."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_wiki = [['<start>', '<start>'] + sent + ['<end>'] for sent in preprocess(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrammer(tokens, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter()\n",
    "bigrams = Counter()\n",
    "trigrams = Counter()\n",
    "\n",
    "for sentence in corpus_wiki:\n",
    "    unigrams.update(sentence)\n",
    "    bigrams.update(ngrammer(sentence))\n",
    "    trigrams.update(ngrammer(sentence, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "# оцените качество также как и раньше\n",
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    \n",
    "    word_pairs = [('<start>', '<start>')] + word_pairs\n",
    "    pred_sent = []\n",
    "    for j in range(2, len(word_pairs)):\n",
    "        \n",
    "        pred = None\n",
    "        \n",
    "        # проверяем, что слова нет в словаре, чтобы не исправлять все слова\n",
    "        if not predict_mistaken(word_pairs[j][1], WORDS):\n",
    "            pred = word_pairs[j][1]\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            # находим кандидатов для исправления\n",
    "            predicted = candidates(word_pairs[j][1], precalculation_dict) \n",
    "        \n",
    "            # берем два предыдущих слова для контекста\n",
    "            prev_word = word_pairs[j-2][1] + ' ' + word_pairs[j-1][1] \n",
    "        \n",
    "            # проверяем есть ли биграмма из двух предыдуших слов, если да - вычисляем триграмму\n",
    "            # если нет, проверяем, есть ли одно из предыдуших слов в униграммах, если да - вычисляем биграмму\n",
    "            # если нет, остается только взять первое по близости\n",
    "            if prev_word not in bigrams:\n",
    "                \n",
    "                prev_word = word_pairs[j-1][1]\n",
    "                if prev_word not in unigrams:\n",
    "                    pred = predicted[0][0]\n",
    "            \n",
    "        \n",
    "                else:\n",
    "                    #\n",
    "                    lm_predicted = []\n",
    "                    for word in predicted:\n",
    "                        bigram = ' '.join([prev_word, word])\n",
    "                        # домножаем полученную метрику для слова на вероятность биграма\n",
    "                        # биграм - предыдущее слово + текущее слово кандидат\n",
    "                        # 1 тут для того чтобы не получались \n",
    "                        lm_predicted.append((word, ((bigrams[bigram]/unigrams[prev_word])))) \n",
    "\n",
    "\n",
    "                    if lm_predicted:\n",
    "\n",
    "                        pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
    "            else:\n",
    "                #\n",
    "                lm_predicted = []\n",
    "                for word in predicted:\n",
    "                    trigram = ' '.join([prev_word, word])\n",
    "                    lm_predicted.append((word, (trigrams[trigram]/bigrams[prev_word]))) \n",
    "    \n",
    "    \n",
    "                if lm_predicted:\n",
    "            \n",
    "                  pred = sorted(lm_predicted, key=lambda x: -x[1])[0][0]\n",
    "\n",
    "        \n",
    "        if pred is None:\n",
    "            pred = word_pairs[j][1]\n",
    "        \n",
    "\n",
    "        \n",
    "        if pred == word_pairs[j][0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((word_pairs[j][0], word_pairs[j][1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if word_pairs[j][0] == word_pairs[j][1]:\n",
    "            total_correct += 1\n",
    "            if word_pairs[j][0] !=  pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if word_pairs[j][0] == pred:\n",
    "                mistaken_fixed += 1\n",
    "    \n",
    "    if not i % 100:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9026830877501649\n",
      "0.04832214765100671\n",
      "0.021080368906455864\n"
     ]
    }
   ],
   "source": [
    "print(correct/total)\n",
    "print(mistaken_fixed/total_mistaken)\n",
    "print(correct_broken/total_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значительно увеличился процент правильных слов (43 < 90), но также значительно меньше слов было подобрано правильно (32 > 5), но и процент неверных исправлений стал меньше (55 > 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
