{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "Реализуйте упрощенную версию byte pair encoding (без предварительного разбивания на слова, которое есть в посте о BPE на медиуме - https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10). \n",
    "\n",
    "Алгоритм должен работать так:\n",
    "строки с текстом разбиваются на отдельные символы и далее в цикле из N итераций: а) считаются статистики встречаемости по парам символов и б) топ-K частотных пар склеиваются в один символ\n",
    "\n",
    "Попробуйте так токенизировать текст с разными параметрами N и K. Проанализируйте словарь уникальных слов для нескольких наборов параметров - сколько уникальных слов получилось, какой токен самый длинный?\n",
    "\n",
    "(5 баллов)\n",
    "\n",
    "Чтобы получить 1 бонусный балл - зафиксируйте получившийся словарь и токенизируйте с помощью него текст, который ранее не встречался в корпусе (возьмите рандомную новость из яндекс новостей например). Проанализируйте насколько хорошо токенизировался текст.\n",
    "\n",
    "*в первом задании можно использовать любые текстовые данные (например корпус lenta.txt - https://github.com/mannefedov/compling_nlp_hse_course/blob/master/data/lenta.txt.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
    "\n",
    "news = open('data/lenta.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
    "    return \" \".join(normalized_text) # склеиваю, чтобы было без предварительного разбиения на слова\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_news = normalize(news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально в vocabulary хочу добавить все символы, затем буду его обновлять найденными топ-k частотными парами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary(text, n, k):\n",
    "    vocab = dict()\n",
    "\n",
    "    for char in text:\n",
    "        if char in vocab:\n",
    "            vocab[char] += 1\n",
    "        else:\n",
    "            vocab[char] = 1   \n",
    "    \n",
    "    pairs = dict()\n",
    "    for step in range(n):\n",
    "        for i in vocab:\n",
    "            for j in vocab:\n",
    "                if i != ' ' and j != ' ':\n",
    "                    temp_pair = i + j\n",
    "                    if temp_pair not in pairs:\n",
    "                        pairs[temp_pair] = text.count(temp_pair)\n",
    "        pairs = {k: v for k, v in sorted(pairs.items(), key=lambda item: item[1], reverse=True)} # https://stackoverflow.com/questions/613183/how-do-i-sort-a-dictionary-by-value\n",
    "    \n",
    "        count = 0    \n",
    "        for pair in pairs:\n",
    "            if count < k:\n",
    "                if pair not in vocab:\n",
    "                    vocab[pair] = pairs[pair]\n",
    "                    count += 1\n",
    "            else:\n",
    "                break  \n",
    "            \n",
    "    max_len = 0\n",
    "    longest_token = ''\n",
    "    for i in vocab:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "            longest_token = i\n",
    "    print(f\"Количество уникальных символов и частотных пар символов в корпусе (с учетом n={n} и k={k}): {len(vocab)}\")\n",
    "    print(f\"Самый длинный токен: {longest_token}\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=5 и k=20): 190\n",
      "Самый длинный токен: ени\n"
     ]
    }
   ],
   "source": [
    "vocabulary_1 = vocabulary(norm_news, 5, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'б': 156501,\n",
       " 'о': 1013087,\n",
       " 'и': 747903,\n",
       " ' ': 1505788,\n",
       " 'у': 215072,\n",
       " 'с': 540120,\n",
       " 'п': 286508,\n",
       " 'ц': 60338,\n",
       " 'к': 307168,\n",
       " 'н': 639161,\n",
       " 'а': 748927,\n",
       " 'д': 288286,\n",
       " 'р': 534412,\n",
       " 'е': 798410,\n",
       " 'з': 145028,\n",
       " 'ч': 118051,\n",
       " 'л': 376818,\n",
       " 'ь': 119048,\n",
       " 'т': 576976,\n",
       " 'м': 280334,\n",
       " 'г': 155743,\n",
       " 'в': 464009,\n",
       " 'я': 164508,\n",
       " 'ш': 53938,\n",
       " 'й': 118139,\n",
       " 'ю': 55058,\n",
       " 'ж': 75632,\n",
       " 'ы': 163989,\n",
       " '1': 15991,\n",
       " '4': 4947,\n",
       " 'х': 83278,\n",
       " 'щ': 38946,\n",
       " '«': 52,\n",
       " '»': 52,\n",
       " '6': 4439,\n",
       " '9': 9227,\n",
       " '2': 11805,\n",
       " '-': 18583,\n",
       " 'э': 26839,\n",
       " 'ф': 36994,\n",
       " '—': 57,\n",
       " 'd': 3735,\n",
       " 'a': 9891,\n",
       " 's': 9363,\n",
       " 'i': 8448,\n",
       " 't': 8397,\n",
       " 'n': 8611,\n",
       " 'e': 13082,\n",
       " 'r': 9296,\n",
       " 'o': 9557,\n",
       " 'f': 2028,\n",
       " 'ъ': 4707,\n",
       " '7': 4085,\n",
       " '5': 7023,\n",
       " '3': 6146,\n",
       " '№': 100,\n",
       " '…': 6,\n",
       " '–': 47,\n",
       " 'l': 4872,\n",
       " 'y': 1676,\n",
       " 'm': 4396,\n",
       " '8': 4120,\n",
       " '0': 18494,\n",
       " 'g': 1928,\n",
       " 'z': 317,\n",
       " 'u': 3653,\n",
       " '.': 452,\n",
       " ',': 2434,\n",
       " 'b': 3071,\n",
       " 'k': 1706,\n",
       " 'w': 2737,\n",
       " 'x': 699,\n",
       " 'p': 3165,\n",
       " 'h': 1926,\n",
       " 'v': 1140,\n",
       " 'j': 298,\n",
       " 'ё': 18,\n",
       " 'c': 6077,\n",
       " '/': 67,\n",
       " 'q': 150,\n",
       " '’': 6,\n",
       " '·': 19,\n",
       " '“': 24,\n",
       " '”': 23,\n",
       " '•': 7,\n",
       " '_': 6,\n",
       " '\\xad': 8,\n",
       " '£': 5,\n",
       " 'ї': 2,\n",
       " 'і': 1,\n",
       " 'ст': 148181,\n",
       " 'ен': 126972,\n",
       " 'ни': 118811,\n",
       " 'ов': 117810,\n",
       " 'ра': 107907,\n",
       " 'но': 107812,\n",
       " 'ро': 106981,\n",
       " 'то': 102137,\n",
       " 'на': 102092,\n",
       " 'ко': 100549,\n",
       " 'пр': 98409,\n",
       " 'ре': 97233,\n",
       " 'по': 96521,\n",
       " 'ан': 90425,\n",
       " 'ос': 87666,\n",
       " 'ер': 86646,\n",
       " 'та': 77887,\n",
       " 'го': 76443,\n",
       " 'ор': 68952,\n",
       " 'ли': 67054,\n",
       " 'ет': 66997,\n",
       " 'не': 66948,\n",
       " 'те': 66661,\n",
       " 'во': 66287,\n",
       " 'ом': 63770,\n",
       " 'де': 63241,\n",
       " 'ва': 62447,\n",
       " 'од': 60565,\n",
       " 'ны': 59935,\n",
       " 'ле': 59830,\n",
       " 'ал': 59491,\n",
       " 'ел': 59475,\n",
       " 'ск': 58488,\n",
       " 'ти': 56278,\n",
       " 'ол': 56046,\n",
       " 'ин': 55618,\n",
       " 'ль': 54637,\n",
       " 'ка': 53913,\n",
       " 'от': 52896,\n",
       " 'ед': 52337,\n",
       " 'ат': 52082,\n",
       " 'ит': 51962,\n",
       " 'об': 51419,\n",
       " 'ри': 51334,\n",
       " 'он': 50474,\n",
       " 'за': 49730,\n",
       " 'ес': 49574,\n",
       " 'ог': 48725,\n",
       " 'ве': 48624,\n",
       " 'ла': 48345,\n",
       " 'со': 47448,\n",
       " 'да': 46856,\n",
       " 'ме': 45653,\n",
       " 'ло': 44862,\n",
       " 'ил': 44630,\n",
       " 'тр': 44580,\n",
       " 'ми': 44512,\n",
       " 'ени': 44390,\n",
       " 'че': 41981,\n",
       " 'ав': 41592,\n",
       " 'ой': 40932,\n",
       " 'тв': 40728,\n",
       " 'ак': 39891,\n",
       " 'ии': 39851,\n",
       " 'нн': 38996,\n",
       " 'ис': 38683,\n",
       " 'ар': 38434,\n",
       " 'ия': 38362,\n",
       " 'ас': 38098,\n",
       " 'до': 37956,\n",
       " 'ки': 37501,\n",
       " 'ви': 37303,\n",
       " 'ост': 35710,\n",
       " 'про': 35356,\n",
       " 'ем': 34944,\n",
       " 'ть': 34747,\n",
       " 'ам': 34443,\n",
       " 'ие': 33935,\n",
       " 'ого': 33407,\n",
       " 'ци': 33118,\n",
       " 'ств': 31894,\n",
       " 'мо': 31378,\n",
       " 'ик': 31036,\n",
       " 'ру': 30665,\n",
       " 'ся': 30560,\n",
       " 'се': 29879,\n",
       " 'нт': 29690,\n",
       " 'ди': 29612,\n",
       " 'пре': 29471,\n",
       " 'из': 28965,\n",
       " 'си': 28753,\n",
       " 'ма': 28685,\n",
       " 'бо': 28418,\n",
       " 'тел': 27971,\n",
       " 'же': 27774,\n",
       " 'сс': 27748,\n",
       " 'вы': 27304,\n",
       " 'ани': 27213,\n",
       " 'им': 27084,\n",
       " 'ий': 26837}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpe(text, vocabulary):\n",
    "    \n",
    "    flag = True\n",
    "    old_text = list(text)     \n",
    "    \n",
    "    while flag:    \n",
    "        new_text = list()   \n",
    "        flag = False        \n",
    "        n = len(old_text)        \n",
    "        i = 0\n",
    "        j = 1\n",
    "        while j < n:\n",
    "            pair = old_text[i] + old_text[j]\n",
    "            if pair in vocabulary:\n",
    "                new_text.append(pair)\n",
    "                i += 2\n",
    "                j += 2\n",
    "                flag = True\n",
    "            else:\n",
    "                new_text.append(old_text[i])\n",
    "                i = j\n",
    "                j += 1\n",
    "        if i == n - 1:\n",
    "            new_text.append(old_text[i])\n",
    "            \n",
    "        old_text = copy.deepcopy(new_text)\n",
    "    return new_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_news_bpe = bpe(norm_news, vocabulary_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"\".join(norm_news_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(norm_news) == len(temp) # проверила, что не забыла никакой символ в bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=5 и k=5): 115\n",
      "Самый длинный токен: ст\n",
      "['б', 'о', 'и', ' ', 'у', ' ', 'с', 'о', 'по', 'ц', 'к', 'и', 'на', ' ', 'и', ' ', 'д', 'р', 'у', 'с', 'к', 'ен', 'и', 'к', ' ', 'з', 'а', 'ко', 'н', 'ч', 'и', 'ли', 'с', 'ь', ' ', 'о', 'т', 'ст', 'у', 'п', 'л', 'ен', 'и', 'е', 'м', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'и', 'я', 'те', 'л', 'ь', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'и', 'с', 'ь', ' ', 'с', ' ', 'с', 'е', 'в', 'ер', 'а', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ч', 'а', 'л', ' ', 'а', 'р', 'т', 'и', 'л', 'л', 'ер', 'и', 'й', 'с', 'к', 'у', 'ю', ' ', 'б', 'ор', 'ь', 'б', 'у', ' ', 'с', ' ', 'к', 'ре', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'а', 'р', 'т', 'и', 'л', 'л', 'ер', 'и', 'й', 'с', 'ко', 'м', ' ', 'б', 'о', 'ю', ' ', 'пр', 'и', 'ни', 'м', 'а', 'ю', 'т', ' ', 'у', 'ч', 'а', 'ст', 'и', 'е', ' ', 'т', 'я', 'ж', 'е', 'л', 'ы', 'е', ' ', 'к', 'а', 'ли', 'б', 'р', 'ы']\n",
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=5 и k=10): 140\n",
      "Самый длинный токен: ст\n",
      "['б', 'о', 'и', ' ', 'у', ' ', 'с', 'о', 'по', 'ц', 'к', 'ин', 'а', ' ', 'и', ' ', 'д', 'р', 'у', 'ск', 'ен', 'и', 'к', ' ', 'за', 'ко', 'н', 'ч', 'и', 'ли', 'с', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'е', 'м', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'и', 'я', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'и', 'с', 'ь', ' ', 'с', ' ', 'с', 'е', 'ве', 'ра', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ч', 'ал', ' ', 'а', 'р', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'у', 'ю', ' ', 'б', 'ор', 'ь', 'б', 'у', ' ', 'с', ' ', 'к', 'ре', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'а', 'р', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'ом', ' ', 'б', 'о', 'ю', ' ', 'пр', 'ин', 'и', 'м', 'а', 'ю', 'т', ' ', 'у', 'ч', 'а', 'ст', 'и', 'е', ' ', 'т', 'я', 'ж', 'ел', 'ы', 'е', ' ', 'ка', 'ли', 'б', 'р', 'ы']\n",
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=5 и k=20): 190\n",
      "Самый длинный токен: ени\n",
      "['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'ч', 'ил', 'ис', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'ма', 'н', 'ц', 'е', 'в', ' ', 'не', 'пр', 'ия', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ч', 'ал', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'у', 'ю', ' ', 'бо', 'р', 'ь', 'б', 'у', ' ', 'с', ' ', 'к', 'ре', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'ом', ' ', 'бо', 'ю', ' ', 'пр', 'ин', 'им', 'а', 'ю', 'т', ' ', 'у', 'ч', 'ас', 'ти', 'е', ' ', 'т', 'я', 'же', 'л', 'ы', 'е', ' ', 'ка', 'ли', 'б', 'р', 'ы']\n",
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=10 и k=5): 140\n",
      "Самый длинный токен: ст\n",
      "['б', 'о', 'и', ' ', 'у', ' ', 'с', 'о', 'по', 'ц', 'к', 'ин', 'а', ' ', 'и', ' ', 'д', 'р', 'у', 'ск', 'ен', 'и', 'к', ' ', 'за', 'ко', 'н', 'ч', 'и', 'ли', 'с', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'е', 'м', ' ', 'г', 'ер', 'м', 'ан', 'ц', 'е', 'в', ' ', 'не', 'пр', 'и', 'я', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'и', 'с', 'ь', ' ', 'с', ' ', 'с', 'е', 'ве', 'ра', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ч', 'ал', ' ', 'а', 'р', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'у', 'ю', ' ', 'б', 'ор', 'ь', 'б', 'у', ' ', 'с', ' ', 'к', 'ре', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'а', 'р', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'ом', ' ', 'б', 'о', 'ю', ' ', 'пр', 'ин', 'и', 'м', 'а', 'ю', 'т', ' ', 'у', 'ч', 'а', 'ст', 'и', 'е', ' ', 'т', 'я', 'ж', 'ел', 'ы', 'е', ' ', 'ка', 'ли', 'б', 'р', 'ы']\n",
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=10 и k=10): 190\n",
      "Самый длинный токен: ени\n",
      "['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'ч', 'ил', 'ис', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'ма', 'н', 'ц', 'е', 'в', ' ', 'не', 'пр', 'ия', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ч', 'ал', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'у', 'ю', ' ', 'бо', 'р', 'ь', 'б', 'у', ' ', 'с', ' ', 'к', 'ре', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'ом', ' ', 'бо', 'ю', ' ', 'пр', 'ин', 'им', 'а', 'ю', 'т', ' ', 'у', 'ч', 'ас', 'ти', 'е', ' ', 'т', 'я', 'же', 'л', 'ы', 'е', ' ', 'ка', 'ли', 'б', 'р', 'ы']\n",
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=10 и k=20): 290\n",
      "Самый длинный токен: тель\n",
      "['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'чи', 'ли', 'с', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'ма', 'н', 'це', 'в', ' ', 'не', 'пр', 'ия', 'тель', ' ', 'при', 'б', 'ли', 'з', 'ив', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ча', 'л', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'у', 'ю', ' ', 'бо', 'р', 'ь', 'б', 'у', ' ', 'с', ' ', 'кр', 'е', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'ом', ' ', 'бо', 'ю', ' ', 'пр', 'ин', 'им', 'а', 'ю', 'т', ' ', 'у', 'ча', 'ст', 'ие', ' ', 'т', 'я', 'же', 'л', 'ые', ' ', 'ка', 'ли', 'б', 'ры']\n",
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=20 и k=5): 190\n",
      "Самый длинный токен: ени\n",
      "['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'ч', 'ил', 'ис', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'ма', 'н', 'ц', 'е', 'в', ' ', 'не', 'пр', 'ия', 'те', 'ль', ' ', 'пр', 'и', 'б', 'ли', 'з', 'и', 'в', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ч', 'ал', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'у', 'ю', ' ', 'бо', 'р', 'ь', 'б', 'у', ' ', 'с', ' ', 'к', 'ре', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'ом', ' ', 'бо', 'ю', ' ', 'пр', 'ин', 'им', 'а', 'ю', 'т', ' ', 'у', 'ч', 'ас', 'ти', 'е', ' ', 'т', 'я', 'же', 'л', 'ы', 'е', ' ', 'ка', 'ли', 'б', 'р', 'ы']\n",
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=20 и k=10): 290\n",
      "Самый длинный токен: тель\n",
      "['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'ко', 'н', 'чи', 'ли', 'с', 'ь', ' ', 'от', 'ст', 'у', 'п', 'ле', 'ни', 'ем', ' ', 'г', 'ер', 'ма', 'н', 'це', 'в', ' ', 'не', 'пр', 'ия', 'тель', ' ', 'при', 'б', 'ли', 'з', 'ив', 'ш', 'ис', 'ь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ча', 'л', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'у', 'ю', ' ', 'бо', 'р', 'ь', 'б', 'у', ' ', 'с', ' ', 'кр', 'е', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'ар', 'ти', 'л', 'ле', 'ри', 'й', 'ск', 'ом', ' ', 'бо', 'ю', ' ', 'пр', 'ин', 'им', 'а', 'ю', 'т', ' ', 'у', 'ча', 'ст', 'ие', ' ', 'т', 'я', 'же', 'л', 'ые', ' ', 'ка', 'ли', 'б', 'ры']\n",
      "Количество уникальных символов и частотных пар символов в корпусе (с учетом n=20 и k=20): 490\n",
      "Самый длинный токен: росси\n",
      "['бо', 'и', ' ', 'у', ' ', 'со', 'по', 'ц', 'ки', 'на', ' ', 'и', ' ', 'д', 'ру', 'ск', 'ен', 'ик', ' ', 'за', 'кон', 'чи', 'ли', 'сь', ' ', 'от', 'ст', 'уп', 'лени', 'ем', ' ', 'ге', 'р', 'ма', 'н', 'це', 'в', ' ', 'не', 'пр', 'ия', 'тель', ' ', 'при', 'бл', 'из', 'ив', 'ши', 'сь', ' ', 'с', ' ', 'се', 'ве', 'ра', ' ', 'к', ' ', 'ос', 'ов', 'ц', 'у', ' ', 'на', 'ча', 'л', ' ', 'ар', 'ти', 'лл', 'ер', 'ий', 'ск', 'ую', ' ', 'бо', 'р', 'ь', 'бу', ' ', 'с', ' ', 'кр', 'е', 'по', 'ст', 'ь', 'ю', ' ', 'в', ' ', 'ар', 'ти', 'лл', 'ер', 'ий', 'ск', 'ом', ' ', 'бо', 'ю', ' ', 'пр', 'ин', 'им', 'аю', 'т', ' ', 'уч', 'ас', 'ти', 'е', ' ', 'т', 'я', 'же', 'л', 'ые', ' ', 'ка', 'ли', 'бр', 'ы']\n"
     ]
    }
   ],
   "source": [
    "for n in [5, 10, 20]:\n",
    "    for k in [5, 10, 20]:\n",
    "        temp_vocab = vocabulary(norm_news, n, k)\n",
    "        norm_news_bpe = bpe(norm_news[:200], temp_vocab)\n",
    "        print(norm_news_bpe[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что с увеличением числа итераций n и с добавлением большего количества топ-k пар словарь увеличивается и в нём появляются более длинные токены. Самый длинный токен, который я увидела \"росси\". Можно заметить, что в результате токенизации начинаются склеиваться не только по 2 символа, но и по 4 (\"тель\" и \"лени\" например)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На доп. балл\n",
    "\n",
    "yandex_news = \"\"\"В России начали разработку программы вживления чипов в мозг человека\n",
    "Правительство России разрабатывает федеральную программу «Мозг, здоровье, интеллект, инновации на 2021—2029 годы» по развитию технологий нейроинтерфейсов.\"\"\"\n",
    "\n",
    "norm_yandex_news = normalize(yandex_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['в', ' ', 'росс', 'ии', ' ', 'на', 'ча', 'ли', ' ', 'раз', 'ра', 'бо', 'т', 'ку', ' ', 'пр', 'ог', 'ра', 'м', 'м', 'ы', ' ', 'в', 'жи', 'влен', 'ия', ' ', 'чи', 'по', 'в', ' ', 'в', ' ', 'мо', 'з', 'г', ' ', 'че', 'ло', 'ве', 'ка', ' ', 'прав', 'ител', 'ьс', 'тво', ' ', 'росс', 'ии', ' ', 'раз', 'ра', 'ба', 'ты', 'ва', 'ет', ' ', 'ф', 'ед', 'ер', 'альн', 'ую', ' ', 'пр', 'ог', 'ра', 'м', 'му', ' ', '«', ' ', 'мо', 'з', 'г', ' ', 'з', 'до', 'ров', 'ь', 'е', ' ', 'ин', 'те', 'лл', 'ек', 'т', ' ', 'ин', 'но', 'ва', 'ции', ' ', 'на', ' ', '2', '0', '2', '1', '—', '2', '0', '2', '9', ' ', 'год', 'ы', ' ', '»', ' ', 'по', ' ', 'ра', 'зв', 'ит', 'ию', ' ', 'те', 'х', 'но', 'ло', 'ги', 'й', ' ', 'не', 'й', 'ро', 'ин', 'тер', 'ф', 'ей', 'сов']\n"
     ]
    }
   ],
   "source": [
    "norm_yandex_news_bpe = bpe(norm_yandex_news, temp_vocab)\n",
    "print(norm_yandex_news_bpe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат получился похожим на то, что произошло с текстом из корпуса: чаще всего токенизируется по два символа, но есть и больше \"влен\", \"росс\", \"ител\", \"прав\" и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "Обучите токенизатор из tokenizers на текстовом корпусе. Рассчитайте статистики для idf по корпусу, используя обученный словарь (разбейте корпус на \"документы\" по новым строкам, каждый \"документ\" токенизируйте, для каждого слова посчитайте, в скольких документах оно встречается и рассчитайте idf разделив общее количество документов на это число, возьмите логарифм от полученного числа). \n",
    "Векторизуйте текст (в мешок слов) аналогично TfidfVectorizer, используя токенизатор и idf статистики (инициализируйте*** пустую матрицу размером (N документов, K слов в словаре) и в цикле по всем документам постепенно заполните ее - токенизируйте документ, рассчитайте TF каждого слова (количество вхождений в документе поделить на общее количество слов в документе), умножьте TF на IDF и, используя индексы слов в словаре, запишите получившееся значение в матрицу)\n",
    "\n",
    "Формулу для TFIDF можете уточнить тут -  https://ru.wikipedia.org/wiki/TF-IDF\n",
    "\n",
    "***Чтобы инициализировать разреженную матрицу используйте scipy:***\n",
    "from scipy.sparse import lil_matrix\n",
    "X = lil_matrix(N, K)\n",
    "\n",
    "Обучите классификатор на полученных векторах и оцените на кросс-валидации. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset_ok.csv')[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>наебалово века, для долбаёбов\\n</td>\n",
       "      <td>INSULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>вся дума в таком же положении😁\\n</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>а в каком месте массовое столкновение? шрайбик...</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>значит ли это, что контроль за вывозом крупног...</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>вам не нужен щеночек? очень хорошие 🐶🥰\\n</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>долбоебы которое пишут эту хрень\\n</td>\n",
       "      <td>INSULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>а я помню, как мы с подружками лазили белым дн...</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>так поднемиесь. если есть кому. всю росию зане...</td>\n",
       "      <td>INSULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>привет если поймёшь\\n</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>осень - значимое время года для руси, так помо...</td>\n",
       "      <td>NORMAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text   label\n",
       "0                        наебалово века, для долбаёбов\\n  INSULT\n",
       "1                       вся дума в таком же положении😁\\n  NORMAL\n",
       "2      а в каком месте массовое столкновение? шрайбик...  NORMAL\n",
       "3      значит ли это, что контроль за вывозом крупног...  NORMAL\n",
       "4               вам не нужен щеночек? очень хорошие 🐶🥰\\n  NORMAL\n",
       "...                                                  ...     ...\n",
       "29995                 долбоебы которое пишут эту хрень\\n  INSULT\n",
       "29996  а я помню, как мы с подружками лазили белым дн...  NORMAL\n",
       "29997  так поднемиесь. если есть кому. всю росию зане...  INSULT\n",
       "29998                              привет если поймёшь\\n  NORMAL\n",
       "29999  осень - значимое время года для руси, так помо...  NORMAL\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tokenizers in /home/yudina/.local/lib/python3.6/site-packages (0.10.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import CharBPETokenizer, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'].to_csv('corpus.txt', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sub = CharBPETokenizer()\n",
    "tok_sub.train('corpus.txt', vocab_size=2000, min_frequency=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на',\n",
       " 'еба',\n",
       " 'ло',\n",
       " 'во</w>',\n",
       " 'ве',\n",
       " 'ка</w>',\n",
       " ',</w>',\n",
       " 'для</w>',\n",
       " 'дол',\n",
       " 'ба',\n",
       " 'ё',\n",
       " 'бо',\n",
       " 'в</w>']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sub.encode(data.loc[0, 'text']).tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tok_text'] = data['text'].apply(lambda x: tok_sub.encode(x).tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vocab = list(tok_sub.get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(terms, docs):\n",
    "    idfs = list()\n",
    "    for term in terms:\n",
    "        count_docs_with_term = sum([1 for doc in docs if term in doc])\n",
    "        if count_docs_with_term == 0:\n",
    "            idf = 0\n",
    "        else:\n",
    "            idf = math.log(len(docs) / count_docs_with_term)\n",
    "        idfs.append(idf)\n",
    "    return idfs    \n",
    "\n",
    "def calculate_tf(terms, doc):   \n",
    "    tfs = list()    \n",
    "    for term in terms:\n",
    "        count_all = 0\n",
    "        count_term = 0\n",
    "        for word in doc:\n",
    "            if word == term:\n",
    "                count_term += 1\n",
    "            count_all += 1\n",
    "        tf = count_term / count_all\n",
    "        tfs.append(tf)\n",
    "    return tfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 2000\n"
     ]
    }
   ],
   "source": [
    "N = len(data['tok_text'])\n",
    "K = len(temp_vocab)\n",
    "print(N, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 8s, sys: 2.52 s, total: 2min 10s\n",
      "Wall time: 2min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_idf = list()\n",
    "idfs = calculate_idf(temp_vocab, data['tok_text'])\n",
    "for i in range(N):    \n",
    "    tfs = calculate_tf(temp_vocab, data.loc[i, 'tok_text'])\n",
    "    tf_idf.append([idfs[j] * tfs[j] for j in range(K)])\n",
    "X = lil_matrix(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(data.label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=50,\n",
       "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
       "              random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"log\", max_iter=50)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88166667, 0.88166667, 0.8825    , 0.8805    , 0.882     ])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, y, scoring=\"f1_micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88133333, 0.88183333, 0.88283333, 0.8805    , 0.88166667])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, y, scoring=\"f1_micro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
